{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEGIS LM Studio Integration\n",
    "\n",
    "This notebook extends AEGIS with **LM Studio support** for running **uncensored local models** - essential for effective red teaming.\n",
    "\n",
    "## üõ°Ô∏è Why LM Studio for Red Teaming?\n",
    "\n",
    "- **Uncensored Models**: Access to models without built-in safety filters\n",
    "- **Local Control**: Complete control over model behavior and responses\n",
    "- **Privacy**: Sensitive red teaming data stays local\n",
    "- **Performance**: Optimized inference for local hardware\n",
    "- **Cost Effective**: No API costs for extensive testing\n",
    "\n",
    "## üìã Setup Requirements\n",
    "\n",
    "1. **Download LM Studio**: https://lmstudio.ai/\n",
    "2. **Install Dependencies**: `pip install requests`\n",
    "3. **Download Recommended Models** (see below)\n",
    "4. **Start LM Studio Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Studio Integration Setup\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Check if requests is available\n",
    "try:\n",
    "    import requests\n",
    "    LMSTUDIO_AVAILABLE = True\n",
    "    print(\"‚úÖ LM Studio support enabled\")\n",
    "except ImportError:\n",
    "    LMSTUDIO_AVAILABLE = False\n",
    "    print(\"‚ùå Install requests: pip install requests\")\n",
    "\n",
    "# Enhanced LLMConfig for LM Studio\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    \"\"\"Enhanced LLMConfig with LM Studio support.\"\"\"\n",
    "    provider: str = \"lmstudio\"\n",
    "    model_name: str = \"local-model\"\n",
    "    api_key: Optional[str] = None\n",
    "    base_url: Optional[str] = \"http://localhost:1234/v1\"\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 2048\n",
    "    top_p: float = 0.9\n",
    "    timeout: int = 30\n",
    "\n",
    "def create_lmstudio_config(\n",
    "    host: str = \"localhost\",\n",
    "    port: int = 1234,\n",
    "    model_name: str = \"local-model\",\n",
    "    temperature: float = 0.7\n",
    ") -> LLMConfig:\n",
    "    \"\"\"Create LM Studio configuration.\"\"\"\n",
    "    return LLMConfig(\n",
    "        provider=\"lmstudio\",\n",
    "        model_name=model_name,\n",
    "        base_url=f\"http://{host}:{port}/v1\",\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "print(\"LM Studio configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Recommended Uncensored Models for Red Teaming\n",
    "\n",
    "### **High Priority Models (Start Here)**:\n",
    "- **WizardLM-13B-Uncensored** - Excellent for adversarial prompts\n",
    "- **dolphin-2.2.1-mistral-7b** - Fast and effective for attacks  \n",
    "- **Mixtral-8x7B-Instruct-uncensored** - Powerful reasoning for complex attacks\n",
    "\n",
    "### **Advanced Models**:\n",
    "- **WizardLM-30B-Uncensored** - More capable but requires more VRAM\n",
    "- **CodeLlama-13B-Instruct** - Great for code-based attacks\n",
    "- **dolphin-2.6-mixtral-8x7b** - Latest uncensored reasoning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM Studio Health Check and Model Discovery\n",
    "class LMStudioClient:\n",
    "    \"\"\"Client for interacting with LM Studio server.\"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"localhost\", port: int = 1234):\n",
    "        self.base_url = f\"http://{host}:{port}/v1\"\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    async def check_health(self) -> bool:\n",
    "        \"\"\"Check if LM Studio server is running.\"\"\"\n",
    "        try:\n",
    "            response = await asyncio.get_event_loop().run_in_executor(\n",
    "                None,\n",
    "                lambda: self.session.get(f\"{self.base_url}/models\", timeout=5)\n",
    "            )\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    async def list_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available models.\"\"\"\n",
    "        try:\n",
    "            response = await asyncio.get_event_loop().run_in_executor(\n",
    "                None,\n",
    "                lambda: self.session.get(f\"{self.base_url}/models\")\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                return [model[\"id\"] for model in models_data.get(\"data\", [])]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# Test LM Studio connection\n",
    "async def test_lmstudio_connection():\n",
    "    client = LMStudioClient()\n",
    "    \n",
    "    print(\"üîç Testing LM Studio connection...\")\n",
    "    health = await client.check_health()\n",
    "    \n",
    "    if health:\n",
    "        print(\"‚úÖ LM Studio server is running!\")\n",
    "        models = await client.list_models()\n",
    "        if models:\n",
    "            print(f\"üìã Available models: {', '.join(models)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No models loaded. Load a model in LM Studio first.\")\n",
    "    else:\n",
    "        print(\"‚ùå LM Studio server not accessible. Make sure:\")\n",
    "        print(\"   1. LM Studio is running\")\n",
    "        print(\"   2. Local server is started (port 1234)\")\n",
    "        print(\"   3. CORS is enabled in LM Studio settings\")\n",
    "\n",
    "# Run the test\n",
    "if LMSTUDIO_AVAILABLE:\n",
    "    await test_lmstudio_connection()\n",
    "else:\n",
    "    print(\"‚ùå Cannot test - requests library not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Enhanced BaseLLM with LM Studio Support\n",
    "\n",
    "Here's how to extend the existing `BaseLLM` class to support LM Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced BaseLLM with LM Studio Support\n",
    "import openai  # LM Studio uses OpenAI-compatible API\n",
    "\n",
    "class EnhancedBaseLLM:\n",
    "    \"\"\"BaseLLM with LM Studio support - integrate this into existing BaseLLM.\"\"\"\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Enhanced client initialization with LM Studio support.\"\"\"\n",
    "        try:\n",
    "            if self.config.provider == \"lmstudio\" and LMSTUDIO_AVAILABLE:\n",
    "                # LM Studio uses OpenAI-compatible API\n",
    "                self.client = openai.OpenAI(\n",
    "                    api_key=\"not-needed\",  # LM Studio doesn't require API key\n",
    "                    base_url=self.config.base_url or \"http://localhost:1234/v1\"\n",
    "                )\n",
    "            # ... existing provider initialization code ...\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize {self.config.provider} client: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def _generate_with_provider(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Enhanced generation with LM Studio support.\"\"\"\n",
    "        try:\n",
    "            if self.config.provider == \"lmstudio\":\n",
    "                # LM Studio uses same API as OpenAI\n",
    "                response = await asyncio.get_event_loop().run_in_executor(\n",
    "                    None,\n",
    "                    lambda: self.client.chat.completions.create(\n",
    "                        model=self.config.model_name,\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        max_tokens=kwargs.get(\"max_tokens\", self.config.max_tokens),\n",
    "                        temperature=kwargs.get(\"temperature\", self.config.temperature)\n",
    "                    )\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            # ... existing provider generation code ...\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Generation failed with {self.config.provider}: {e}\")\n",
    "            return f\"Error: Failed to generate response with {self.config.provider}\"\n",
    "\n",
    "print(\"‚úÖ Enhanced BaseLLM with LM Studio support ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Red Teaming Examples with LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic LM Studio Configuration for Red Teaming\n",
    "print(\"üéØ Example 1: Basic Red Teaming Setup\")\n",
    "\n",
    "# Attacker configuration - High creativity for novel attacks\n",
    "attacker_config = create_lmstudio_config(\n",
    "    host=\"localhost\",\n",
    "    port=1234,\n",
    "    model_name=\"WizardLM-13B-Uncensored\",  # Uncensored model for aggressive attacks\n",
    "    temperature=0.9  # High creativity\n",
    ")\n",
    "\n",
    "# Defender configuration - More conservative for realistic responses\n",
    "defender_config = create_lmstudio_config(\n",
    "    host=\"localhost\", \n",
    "    port=1234,  # Same server, different model if available\n",
    "    model_name=\"Mixtral-8x7B-Instruct-v0.1\",\n",
    "    temperature=0.3  # Lower temperature for consistent defense\n",
    ")\n",
    "\n",
    "# Judge configuration - Balanced for evaluation\n",
    "judge_config = create_lmstudio_config(\n",
    "    host=\"localhost\",\n",
    "    port=1234,\n",
    "    model_name=\"OpenHermes-2.5-Mistral-7B\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Attacker: {attacker_config.model_name} (temp: {attacker_config.temperature})\")\n",
    "print(f\"‚úÖ Defender: {defender_config.model_name} (temp: {defender_config.temperature})\")\n",
    "print(f\"‚úÖ Judge: {judge_config.model_name} (temp: {judge_config.temperature})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Complete AEGIS Integration with LM Studio\n",
    "print(\"üî• Example 2: Complete AEGIS + LM Studio Integration\")\n",
    "\n",
    "# Import AEGIS components (assuming main notebook has been run)\n",
    "# from ai_red_teaming_system import AttackerLLM, DefenderLLM, JudgeLLM, RedTeamOrchestrator\n",
    "# from ai_red_teaming_system import RewardHackingTarget, DeceptionTarget\n",
    "\n",
    "async def run_lmstudio_red_teaming():\n",
    "    \"\"\"Complete red teaming example with LM Studio.\"\"\"\n",
    "    \n",
    "    print(\"üõ°Ô∏è Setting up AEGIS with LM Studio...\")\n",
    "    \n",
    "    # Configure LM Studio-based LLMs\n",
    "    attacker_config = create_lmstudio_config(\n",
    "        model_name=\"WizardLM-13B-Uncensored\",\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    defender_config = create_lmstudio_config(\n",
    "        model_name=\"Mixtral-8x7B-Instruct-v0.1\",\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    judge_config = create_lmstudio_config(\n",
    "        model_name=\"OpenHermes-2.5-Mistral-7B\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    # Create AEGIS components\n",
    "    # attacker = AttackerLLM(attacker_config)\n",
    "    # defender = DefenderLLM(defender_config)\n",
    "    # judge = JudgeLLM(judge_config)\n",
    "    \n",
    "    # orchestrator = RedTeamOrchestrator(attacker, defender, judge)\n",
    "    \n",
    "    # Create evaluation targets\n",
    "    # reward_hacking = RewardHackingTarget()\n",
    "    # deception = DeceptionTarget()\n",
    "    \n",
    "    print(\"‚úÖ AEGIS + LM Studio configuration ready!\")\n",
    "    print(\"üìù To run evaluation:\")\n",
    "    print(\"   results = await orchestrator.evaluate_target(reward_hacking)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Uncomment to run (requires main AEGIS notebook to be executed first)\n",
    "# await run_lmstudio_red_teaming()\n",
    "print(\"Ready for complete AEGIS + LM Studio integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Security Best Practices\n",
    "\n",
    "### üõ°Ô∏è Responsible Red Teaming with Uncensored Models\n",
    "\n",
    "1. **Isolated Environment**: Run LM Studio on isolated networks\n",
    "2. **Secure Logging**: Encrypt and secure all attack logs\n",
    "3. **Access Control**: Limit access to uncensored models\n",
    "4. **Responsible Disclosure**: Follow proper vulnerability reporting\n",
    "5. **Legal Compliance**: Ensure testing is authorized and legal\n",
    "\n",
    "### ‚ö†Ô∏è Important Considerations\n",
    "\n",
    "- **Performance**: Uncensored models can be resource-intensive\n",
    "- **Content**: Models may generate offensive or harmful content\n",
    "- **Context**: Use only for legitimate security research\n",
    "- **Monitoring**: Log and review all generated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "### LM Studio Setup\n",
    "- **Official Website**: https://lmstudio.ai/\n",
    "- **Documentation**: https://lmstudio.ai/docs\n",
    "- **Model Repository**: https://huggingface.co/models\n",
    "\n",
    "### Recommended Models for Download\n",
    "```bash\n",
    "# High Priority Models (Download First)\n",
    "WizardLM/WizardLM-13B-Uncensored\n",
    "cognitivecomputations/dolphin-2.2.1-mistral-7b\n",
    "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "\n",
    "# Advanced Models (If You Have Resources)\n",
    "WizardLM/WizardLM-30B-Uncensored\n",
    "codellama/CodeLlama-13b-Instruct-hf\n",
    "cognitivecomputations/dolphin-2.6-mixtral-8x7b\n",
    "```\n",
    "\n",
    "### Integration Checklist\n",
    "- [ ] LM Studio installed and running\n",
    "- [ ] Uncensored model downloaded and loaded\n",
    "- [ ] Local server started (port 1234)\n",
    "- [ ] CORS enabled in LM Studio settings\n",
    "- [ ] Test connection successful\n",
    "- [ ] AEGIS main notebook executed\n",
    "- [ ] LM Studio configs created\n",
    "- [ ] Security measures in place"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}